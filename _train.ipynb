{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf3421c79544edca9f3baf8761ab1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8,013,769 sample texts from data/owt_tokenized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukaemon/miniconda3/envs/topk_sae/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "\"\"\"dimension annotation\n",
    "b: batch\n",
    "t: token position\n",
    "d: d_model\n",
    "v: model token vocab size\n",
    "l: SAE n latent\n",
    "k: topk\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import HookedTransformer\n",
    "from sparse_autoencoder.model import Autoencoder, TopK\n",
    "from sparse_autoencoder.loss import autoencoder_loss\n",
    "from tqdm import tqdm\n",
    "\n",
    "from openwebtext import load_owt, sample\n",
    "\n",
    "\n",
    "ds = load_owt()\n",
    "gpt2 = HookedTransformer.from_pretrained(\"gpt2\", center_writing_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer = 8\n",
    "k = 32\n",
    "batch_size = 64\n",
    "n_batch = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_batch):\n",
    "        batch = sample(ds, batch_size)\n",
    "        loss, cache = gpt2.run_with_cache(batch, return_type=\"loss\")\n",
    "        act_btd = cache[utils.get_act_name(\"resid_post\", layer=target_layer)]\n",
    "        act_bd = act_btd[:, -1]\n",
    "        train_data.append(\n",
    "            (batch.detach().cpu(), act_bd.detach().cpu(), loss.detach().cpu())\n",
    "        )\n",
    "    \n",
    "del(gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step(0): loss = 0.772\n",
      "step(1): loss = 0.765\n",
      "step(2): loss = 0.760\n",
      "step(3): loss = 0.754\n",
      "step(4): loss = 0.745\n",
      "step(5): loss = 0.738\n",
      "step(6): loss = 0.730\n",
      "step(7): loss = 0.718\n",
      "step(8): loss = 0.710\n",
      "step(9): loss = 0.707\n",
      "step(10): loss = 0.701\n",
      "step(11): loss = 0.684\n",
      "step(12): loss = 0.670\n",
      "step(13): loss = 0.667\n",
      "step(14): loss = 0.664\n",
      "step(15): loss = 0.657\n",
      "step(16): loss = 0.648\n",
      "step(17): loss = 0.641\n",
      "step(18): loss = 0.631\n",
      "step(19): loss = 0.626\n",
      "step(20): loss = 0.624\n",
      "step(21): loss = 0.613\n",
      "step(22): loss = 0.615\n",
      "step(23): loss = 0.592\n",
      "step(24): loss = 0.599\n",
      "step(25): loss = 0.602\n",
      "step(26): loss = 0.584\n",
      "step(27): loss = 0.587\n",
      "step(28): loss = 0.586\n",
      "step(29): loss = 0.578\n",
      "step(30): loss = 0.567\n",
      "step(31): loss = 0.566\n",
      "step(32): loss = 0.559\n",
      "step(33): loss = 0.559\n",
      "step(34): loss = 0.534\n",
      "step(35): loss = 0.533\n",
      "step(36): loss = 0.539\n",
      "step(37): loss = 0.519\n",
      "step(38): loss = 0.505\n",
      "step(39): loss = 0.531\n",
      "step(40): loss = 0.527\n",
      "step(41): loss = 0.502\n",
      "step(42): loss = 0.522\n",
      "step(43): loss = 0.507\n",
      "step(44): loss = 0.506\n",
      "step(45): loss = 0.518\n",
      "step(46): loss = 0.482\n",
      "step(47): loss = 0.504\n",
      "step(48): loss = 0.497\n",
      "step(49): loss = 0.480\n",
      "step(50): loss = 0.486\n",
      "step(51): loss = 0.467\n",
      "step(52): loss = 0.456\n",
      "step(53): loss = 0.460\n",
      "step(54): loss = 0.475\n",
      "step(55): loss = 0.457\n",
      "step(56): loss = 0.459\n",
      "step(57): loss = 0.441\n",
      "step(58): loss = 0.424\n",
      "step(59): loss = 0.435\n",
      "step(60): loss = 0.449\n",
      "step(61): loss = 0.443\n",
      "step(62): loss = 0.424\n",
      "step(63): loss = 0.443\n"
     ]
    }
   ],
   "source": [
    "n_latents = 2**15\n",
    "n_inputs = 768\n",
    "act_fn = TopK(k)\n",
    "\n",
    "device = utils.get_device()\n",
    "sae = Autoencoder(n_latents, n_inputs, act_fn, tied=True, normalize=True).to(device)\n",
    "optimizer = torch.optim.Adam(sae.parameters(), lr=5e-4)\n",
    "\n",
    "for i, (_, act, _) in enumerate(train_data):\n",
    "    act = act.to(device)\n",
    "    _, latent, recon = sae(act)\n",
    "    loss = autoencoder_loss(recon, act, latent, l1_weight=0)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    print(f\"step({i}): loss = {loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topk_sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
